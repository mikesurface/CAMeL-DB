\section{Integration}

\sean{Add paragraph on "frequentist" (MV) versus "Bayesian" approach}
One of the difficulties in relying on information from a crowd of sources is the possibility of a high degree of noise due to unreliable and in some cases even malicious sources.  One of the standard procedures for increasing quality control is to increase the redundancy of questions.  By asking the same question to multiple sources and aggregating the answers, we can achieve a higher probability of a good answer.

In many cases, it suffices to collect, say, 3 or 5 votes on each question and use the majority opinion.  There are potential scenarios in which this ceases to be an effective strategy.  If the probability of receiving low quality work is equal to or greater than that of receiving higher quality, it's detrimental to treat every vote of equal merit.  Confusing or difficult questions can also cause conflict among the workers and result in a mix of answers.  Taking the deterministic mode results in a loss of information about the controversy of the question, information which may prove useful in applications such as sentiment analysis or opinion-dominated questions.

Thus we are led to a desire to manifest the crowd response probabilistically, weighing votes proportionately and making decisions when conflicted on a question.  We implement two approaches for this data integration task, drawing separately from probability theory and belief theory.  The first maintains a single probability function, establishing a prior based on the machine's labeling, and updating the posterior using Bayes's Rule.  Alternatively, we combine the Turker response in the absence of the machine prior using Dempster-Shafer theory.  Both methods require an identification of the level of quality of each individual Turker.  We describe previous work that we've leveraged in the next section before outlining our two integration methods.\sean{Still may want to add Halpern and Fagin reference.}

\subsection{Evaluating Turker Quality}

Amazon Mechanical Turk provides no working system for maintaining the quality and reliability of their workforce and it is generally up to the Requester to ascertain such values on their own.  The simplest system, known as "honey potting", is to carefully intermix questions for which the answer is known in advance and judge Turker performance against the gold standard.  While generally effective, it lacks robustness and is defeatable to smart enough Turkers that can recognize them over time.  More sophisticated methods estimate quality an unsupervised manner by judging each Turker's level of agreement with the mean set of answers.  Examples include Bayesian \cite{citeulike:9437699, DBLP:journals/jmlr/RaykarY12} methods and an approach using majority vote and expectation maximization \cite{Ipeirotis:2010:QMA:1837885.1837906}.

We focus on a modified version of latter, attributable to Dawid and Skene \cite{1979}, for implementation into \sysName .  For each question the EM algorithm takes a set of answers $a_{1}$,...,$a_{N}$ provided by N Turkers assumed to be drawn from a categorical distribution.  Associated with each Turker is a latent "confusion matrix" $\pi^{k}_{ij}$ that designates the probability the $k^{th}$ Turker will provide label $j$ when true answer is $i$.  Our modification simplifies to a binary accuracy variable $\pi^(k)$, which represents probability they will correctly label a question with the true answer.  The goal of Dawid and Skene's EM algorithm is to recover $\pi^{k}$ in the presence of the answers $a^{m}_{1}$,...,$a^{m}_{k}$ for a set of questions $m \in M$.

In order to obtain a sufficient number of answers to similar questions by, HITs are designed in higher cost blocks.  The single task of supplying a label to a token is worth around \$0.01.  HITs are packaged in groups of 10 questions at \$0.10 each.  This ensures that if $K$ Turkers answer the HIT, relative performance can be judged across all 10 questions.

The algorithm initializes each Turker's accuracy to 1.  It takes a majority vote among the answers to each question to define an initial answer set.  Based on this agreed upon answer set, each Turker's accuracy $\pi^{k}$ is computed.  Another majority vote weighted by $\pi^{k}$ determines a possibly different answer set.  The Turker accuracies are re-computed.  This process continues until convergence in both the "true" answer set and the $\pi^{k}$ accuracies.

Let us take a moment to define precisely how we interpret Turker quality in the context of results of the EM algorithm.  While the Dawid \& Skene approach ultimately is calculated as correct or incorrect accuracies from a set of questions, we assume a different characteristic behavior associated with this score.  Instead of the quality being a measure of whether we believe the Turker is "correct" or not, we take quality to be a measure of \textit{reliability}.  The quality score models the probability the Turker knows the correct answer and selects accordingly, while the inverse is the probability of a \textit{random guess} from the set of possible answers.  The two approaches in the next section tackle the problem of combining responses once we have an estimate of the Turkers' quality or reliability. 

\subsection{Two Approaches to Integration}
%~ \eat{
%~ The reason for submitting to belief theory as our main tool in the aggregation of the Turkers and machine is that it provides a natural framework arriving at a posterior distribution composed of various pieces of evidence.  While the roots of belief theory first centered around the Dempster-Shafer model, much criticism has been laid upon the model for turning up erroneous or inaccurate results.  Halpern and Fagin \cite{DBLP:journals/ai/HalpernF92} argue this is purely from a misuse of appropriating one interpretation for another.  The first view of belief function one can take is that of a generalized probability function, starting with a prior probability and updating as new evidence comes along to arrive at a conditional posterior.  On the other hand, viewing belief functions as evidence themselves leads one to use Dempster's Rule of Combination.  One presents the \textit{updating} of evidence while they other presents the \textit{combining}.  One utilizes a prior while the other does not.
%~ 
%~ We use this as inspiration for studying two different approaches to aggregating humans and machines akin to the differing interpretations.  In our Bayesian formulation, the CRF marginal distribution is used as a prior and \textit{updated} based on Turker responses.  Using an alternative Dempster-Shafer model, we forego the use of a prior and \textit{combine} Turker responses using Dempster's Rule of Combination.  
%}



\subsubsection{Bayesian Conditional Probability}

The fundamental assumption taken with the Bayesian model is that the ML extracted values present a serviceable prior to the model.  For a well-trained model, this view is that the machine is \textit{close enough} and the crowd's response is presented as a tweak, pushing its decision in one direction or another.  One interpretation is of the machine as a regularizer and the more peaked any aspect of the machine distribution is the more impact the prior plays and consequently the greater trust is placed in the original model.

Let $A^{n}_{1}$,...,$A^{n}_{K}$ be a set of categorical random variables corresponding to answers received from $K$ Turkers for question $n$.  The CRF's prior, a random variable $L$ which also follows a categorical distribution over the label space, is our current estimate of the true distribution of labels fora specific token.  The integration problem is to find the posterior $P(L^{n}|A^{n}_{1}$,...,$A^{n}_{K})$ conditioned on the answers provided by the Turkers.  This can be found using Bayes's Rule:     

\begin{equation}
P(L^{n}|A^{n}_{1},...,A^{n}_{K}) = \frac{P(A^{n}_{1},...,A^{n}_{K}|L^{n})P(L^{n})}{P(A)}
\end{equation}

We make the simplifying assumption without loss of generality that marginal probability of any set of answers $P(A)$ is uniform and only focus on the numerator.  It's already been stated that $P(L)$ is just the CRF's marginal probability before considering any new evidence.  The evidence term can be modeled with the same assumption that was made in using Dawid and Skene to estimate Turker quality,

\begin{equation}
\label{eq:independence}
P(A^{n}_{1},...,A^{n}_{K}|L^{n}) = \prod_{k}P(A^{n}_{k}|L^{n})
\end{equation}

\begin{equation}
\label{eq:bayes_evidence}
P(A^{n}_{k}=a|L^{n}=l) = |\mathbbm{1}_{{a}\neq l} - Q_{k}| + |\mathbbm{1}_{a=l}-Q_{k}|*\frac{1}{|L|}
\end{equation}

where $a$ and $l$ are values drawn from the label space and $Q_{k}$ is the quality of the k$^{th}$ worker.  Equation~\ref{eq:independence} follows from all Turker answers being independent of each other and equation~\ref{eq:bayes_evidence} simply restates our assumption about the use of Turkery quality $Q_{k}$.  The first term on the right hand side is the probability the Turker is reliable and answers the question truthfully.  The second term incorporates the probability they are unreliable or a spammer and through \textit{random guessing} get the question correct anyways with probability $1/|L|$, $|L|$ being the number of possible labels.
 
The full model is

\begin{align}
\label{eq:full_bayes}
P(&L^{n}=l|A^{n}_{1}=a_{1},...,A^{n}_{K}=a_{k}) = \\
&P(L^{n}=l)\prod_{k}\big(|\mathbbm{1}_{{a_{k}}\neq l} - Q_{k}|  + |\mathbbm{1}_{a_{k}=l}-Q_{k}|*\frac{1}{|L|}\big)
\end{align}

Using equation~\ref{eq:full_bayes} for all possible labels $l$ and renormalizing produces a new posterior distribution accounting for both the initial ML extracted result and evidence gathered from the crowd.  The product can be extended and updated as new evidence comes in over time.  While currently evidence is designed to come from the crowd in \sysName , there is no explicit restriction preventing future updates from incorporating evidence from a number of different extractions as well as the crowd.  We conclude this section with an explicit example.

\sean{Probably want to change these numbers.  What differences between DS and Bayes do I want to exhibit by using specific numbers?}
EXAMPLE 1. \textit{
Assume a binary question is answered by two Turkers.  Turker A has quality 0.8 and answers label 0, while Turker B has quality 0.6 and answers label 1.  The prior CRF marginal probability over \{0,1\} is \{0.3,0.7\}.  We want to ascertain the combined distribution for the label $L$.  According to equation~\ref{eq:full_bayes}, 
%\begin{equation}
\begin{align}
P(L=0|A,B) &= \frac{1}{Z}P(L=0)P(L=0|A)P(L=0|B)\\
	        &= (0.4)*(0.9)*(0.2)*\frac{1}{Z}\\
	        &= .072*\frac{1}{Z}\\
P(L=1|A,B) &= \frac{1}{Z}P(L=1)P(L=1|A)P(L=1|B)\\
	        &= (0.6)*(0.1)*(0.8)*\frac{1}{Z}\\
	        &= .048*\frac{1}{Z}
\end{align}
%\end{equation}
After combining and normalizing, the final distribution over \{0,1\} is \{0.6,0.4\}.  While the CRF originally favored label 1, the new distribution favors label 0.
}

\subsubsection{Dempster-Shafer Evidential Combination}
%~ \eat{
%~ A viable alternative is to exhibit no faith in the machine's initial marginal calculation.  After all, one could argue that by selecting only the most uncertain tokens that metric loses its value.  
%~ }

Without explicit reference to the CRF prior, we're left with the task of \textit{combining} disparate evidence from a group of Turkers.  This can be accomplished using Dempster's Rule of Combination, which operates over a set of mass functions.  Mass functions differ from probability functions by relaxing the Kolmogorov axiom that functions must sum to 1.

%~ While the Bayesian approach was inspired by an alternative interpretation of belief functions, the actual implementation is still a probability function through and through, with all of Kolmogorov's axioms defining a probability function still holding.  For evidential combination, however, we leverage the full power of belief theory and relax some of those axioms to map to a set of belief functions.  

%~ The main difference between a belief function and a probability function is that probability functions are defined only over the \textit{measurable} subsets of a set while belief functions are defined over \textit{all} subsets (the power set) of a set \cite{shafer1976mathematical}.

We now describe mapping of the Turker data these mass functions.  Like with the Bayesian approach, our confidence in them getting the answer correct is reflected in their Quality score.  The mass function $m(a_{k})$ gets assigned the score $Q_{k}$.  Let $\mathcal{A}$ be the set of all possible labels ${1,2,...,L}$.  Intuitively, $m(\mathcal{A})$ is the mass associated with a random guess and all $L$ labels being equally likely.  We assume in this framework that Turkers are either reliable, getting the answer correct with belief score $Q_{k}$, or unreliable, reflected in a random guess with belief score $1-Q_{k}$.  Explicitly, for a Turker $k$ with provided answer $a_{k}$:

\begin{equation}
m^{n}(2^{L}) = 0
\end{equation}
\begin{equation}
\label{eq:mass1}
m^{n}(a_{k}) = Q_{k}
\end{equation}
\begin{equation}
\label{eq:mass2}
m^{n}(\mathcal{A}) = 1-Q_{k}
\end{equation}

The first equation simply states that initialize all mass functions to zero before setting the two values below.  The mass function $m(\mathcal{A})$ has no meaning in standard probability theory, as the set of all outcomes is not a measurable in the probabilistic sense.  We use it mainly as bookkeeping for the uncertainty in the result before normalizing it out when the aggregation computation is completed.  The set of mass functions from multiple Turkers can be combined using Dempster's Rule of Combination between Turker 1 and Turker 2 for each set $A\in2^{L}$:

\begin{equation}
\begin{split}
\label{eq:DS_combo}
m_{0,1}(A) &=(m_{1}\oplus m_{2})(A)\\
                   &=\frac{1}{1-K} \sum_{B\cap C=A\neq\emptyset} m_{1}(B)m_{2}(C)
\end{split}
\end{equation}

\begin{equation}
K=\sum_{B\bigcap C=\emptyset}m_{1}(B)m_{2}(C)
\end{equation}

The procedure is to map all HIT responses to mass functions and combine them one-by-one in turn to produce a single combined mass function.  Any remaining uncertainty in $m_{comb}(\mathcal{A})$ is added to all the singleton functions and re-normalized to produce a single probability function.  The original belief formulation is maintained in \sysName for easy combination if new evidence arrives at a later time.

\textit{
EXAMPLE 2. Given the same Turkers and answers from EXAMPLE 1, Dempster's Rule of Combination may also be used to combine them.  First, we map them to mass functions using equations~\ref{eq:mass1} and ~\ref{eq:mass2}:
\begin{align*}
m_{A}(0) = 0.8,  m_{A}(1) = 0,  m_{A}(0,1) = 0.2\\
m_{B}(0) = 0,  m_{B}(1) = 0.6,  m_{B}(0,1) = 0.4
\end{align*}
We apply equation~\ref{eq:DS_combo} to combine Turkers A and B.
\begin{align*}
m_{A,B}(0) = m_{A}(0)*m_{B}(0,1) = 0.32\\
m_{A,B}(1) = m_{B}(1)*m_{A}(0,1) = 0.12\\
m_{A,B}(0,1) = m_{A}(0,1)*m_{B}(0,1) = 0.08
\end{align*}
To convert to a probability distribution, we add $m_{A,B}(0,1)$ to each of the individual components and normalize.  The final distribution over \{0,1\} is \{0.66,0.34\}.  The contrasts with the result of EXAMPLE 1 by the exclusion of a machine prior.
}

While we introduce Dempster-Shafer theory here in the context of our simpler one-answer-per-question framework currently found in \sysName , it is not to be taken in contrast with its Bayesian counterpart, but as a generalization of it.  The method will become more powerful in future work when we plan to extend functionality to allow the Turkers to provide more than one response per question when uncertain.  Reasoning over such fuzzy sets exemplifies the real power for using belief theory over probability theory.

\subsection{Quantifying Turker Performance}

Even human computation is not perfect.  The previous section looked at ways to combine Turker answers probabilistically to arrive at a final result that is not deterministic.  This is useful for when there is controversy or confusion elicited over the answers of a question.  We use the entropy of the final label distribution to arrive at confidence value for each question.  Depending on the required accuracy of the application, a threshold on the confidence may be placed to assure only the highest quality results make it through.  In the experiments we highlight Receiver Operating Characteristic (ROC) curves that measure performance vs. answer recall.  Answers not making the cut may have their questions re-submitted to attain more information in discerning the result. 

\subsection{Probabilistic Integration}

\begin{algorithm}[fillcomment]
\label{alg:integration}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{Array of turker answers A,\\
ML prior label dist. M,\\
CRF model C,\\
Unlabeled document d,\\
Token t}
\Output{Labeled document d\_labeled}
\BlankLine
\CommentSty{//Estimate Turker qualities from answers}\;
Q = Dawid\_Skene(A)\;
\CommentSty{//Compute posterior distribution of answers}\;
\If{Bayesian}{
	combination = Bayesian\_integrate(M,A,Q)\;
}
\If{Dempster-Shafer}{
	combination = DS\_integrate(A,Q)\;
}
\CommentSty{//Integrate back into model}\;
label = index\_of\_max(combination)\;
d\_labeled = constrained\_Viterbi(C,d,t,label)\;

\caption{Probabilistic integration through constrained Viterbi.}
\end{algorithm}

Not only does \sysName have the ability to aggregate answers from multiple sources, but also the ability to reinsert the resulting distribution back into the CRF. Since the underlying architecture of the system is a CRF, the dependence properties of each field are made explicit and re-running the inference algorithm has the potential to change surrounding fields as well.  This "constrained inference" substitutes the aggregated marginal distribution of a token in for the computed transition probabilities.  This highlights a very strong advantage of \sysName system, in that large errors can be corrected by small, incremental changes.

Algorithm~\ref{alg:integration} shows the basic outline of our probabilistic integration scheme.  Turker answers pulled from AMT are a set of labels for token $t$ from document $d$.  They're used in the Dawid \& Skene EM method to estimate the Turkers' individual quality measures.  Depending on the integration technique chosen, either Bayesian or Dempster-Shafer is used to compute the final posterior distribution over $t$'s possible labels.  The max likelihood label is used in the constrained Viterbi function over $d$.  This function computes Viterbi in a similar manner as the original, except that all paths not passing through the max likelihood label for $t$ are set to zero.

