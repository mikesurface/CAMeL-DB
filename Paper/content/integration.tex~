\section{Integration}

\sean{Add paragraph on "frequentist" (MV) versus "Bayesian" approach}
One of the difficulties in relying on information from a crowd of sources is the possibility of a high degree of noise due to unreliable and in some cases even malicious sources.  One of the standard procedures for increasing quality control is to increase the redundancy of questions.  By asking the same question to multiple sources and aggregating the answers, one achieves a higher probability of a high quality answer.

In many cases, it suffices to collect 3 or 5 votes on each question and use the majority opinion.  There are potential scenarios in which this ceases to be an effective strategy.  If the probability of receiving low quality work is equal to or greater than that of receiving higher quality, it is detrimental to treat every vote of equal merit.  Confusing or difficult questions may also cause conflict among the workers and result in a mix of answers.  Taking the deterministic mode results in a loss of information about the controversy of the question, information which may prove useful in applications such as sentiment analysis or opinionated questions.

Thus we are led to a desire to manifest the crowd response probabilistically, weighting votes proportionately and making decisions when it is conflicted on a question.  We develop a principled framework rooted in belief theory for combining crowd data.  There are two fundamentally different ways of interpreting belief theory \cite{DBLP:journals/ai/HalpernF92}, that of a generalized probability distribution (Bayesian) and as a means to combine different forms of evidence (Dempster-Shafer).  We show these to be equivalent if crowd answers are mapped to a certain distribution based on the quality of each worker.  We describe our method for uncovering this quality in the next section.

\subsection{Evaluating Turker Quality}

Amazon Mechanical Turk provides no working system for maintaining the quality and reliability of their workforce and it is generally up to the Requester to ascertain such values on their own.  The simplest system, known as "honey potting", is to carefully intermix questions for which the answer is known in advance and judge Turker performance against the gold standard.  While generally effective, it lacks robustness and is defeatable to smart enough Turkers that can recognize them over time.  More sophisticated methods estimate quality an unsupervised manner by judging each Turker's level of agreement with the mean set of answers.  Examples include Bayesian \cite{citeulike:9437699, DBLP:journals/jmlr/RaykarY12} methods and an approach using majority vote and expectation maximization \cite{Ipeirotis:2010:QMA:1837885.1837906}.

We focus on a modified version of latter, attributable to Dawid and Skene \cite{1979}, for implementation into \sysName .  For each question the EM algorithm takes a set of answers $a_{1}$,...,$a_{N}$ provided by N Turkers assumed to be drawn from a categorical distribution.  Associated with each Turker is a latent "confusion matrix" $\pi^{k}_{ij}$ that designates the probability the $k^{th}$ Turker will provide label $j$ when true answer is $i$.  Our modification simplifies to a binary accuracy variable $\pi^(k)$, which represents probability they will correctly label a question with the true answer.  The goal of Dawid and Skene's EM algorithm is to recover $\pi^{k}$ in the presence of the answers $a^{m}_{1}$,...,$a^{m}_{k}$ for a set of questions $m \in M$.

In order to obtain a sufficient number of answers to similar questions by, HITs are designed in higher cost blocks.  The single task of supplying a label to a token is worth around \$0.01.  HITs are packaged in groups of 10 questions at \$0.10 each.  This ensures that if $K$ Turkers answer the HIT, relative performance can be judged across all 10 questions.

The algorithm initializes each Turker's accuracy to 1.  It takes a majority vote among the answers to each question to define an initial answer set.  Based on this agreed upon answer set, each Turker's accuracy $\pi^{k}$ is computed.  Another majority vote weighted by $\pi^{k}$ determines a possibly different answer set.  The Turker accuracies are re-computed.  This process continues until convergence in both the "true" answer set and the $\pi^{k}$ accuracies.  The full pseudo-code can be found in the appendix.

\subsection{Two Views of Belief Theory for Aggregation}

The reason for submitting to belief theory as our main tool in the aggregation of the Turkers and machine is that it provides a natural framework arriving at a posterior distribution composed of various pieces of evidence.  While the roots of belief theory first centered around the Dempster-Shafer model, much criticism has been laid upon the model for turning up erroneous or inaccurate results.  Halpern and Fagin \cite{DBLP:journals/ai/HalpernF92} argue this is purely from a misuse of appropriating one interpretation for another.  The first view of belief function one can take is that of a generalized probability function, starting with a prior probability and updating as new evidence comes along to arrive at a conditional posterior.  On the other hand, viewing belief functions as evidence themselves leads one to use Dempster's Rule of Combination.  One presents the \textit{updating} of evidence while they other presents the \textit{combining}.  One utilizes a prior while the other does not.

We use this as inspiration for studying two different approaches to aggregating humans and machines akin to the differing interpretations.  In our Bayesian formulation, the CRF marginal distribution is used as a prior and \textit{updated} based on Turker responses.  Using an alternative Dempster-Shafer model, we forego the use of a prior and \textit{combine} Turker responses using Dempster's Rule of Combination.  


\subsubsection{Bayesian Conditional Probability}

The fundamental assumption taken with the Bayesian model is that the ML extracted values present a serviceable prior to the model.  For a well-trained model, this view is that the machine is \textit{close enough} and the crowd's response is presented as a tweak, pushing its decision in one direction or another.  One interpretation is of the machine as a regularizer and the more peaked any aspect of the machine distribution is the more impact the prior plays and consequently the greater trust is placed in the original model.

Let $A^{n}_{1}$,...,$A^{n}_{K}$ be a set of categorical random variables corresponding to answers received from $K$ Turkers for question $n$.  The CRF's prior, a random variable $L$ which also follows a categorical distribution over the label space, is our current estimate of the true distribution of labels for the token in question.  The aggregation problem is to find the posterior $P(L^{n}|A^{n}_{1}$,...,$A^{n}_{K})$ conditioned on the answers provided by the Turkers.  This can be found using Bayes's Rule:     

\begin{equation}
P(L^{n}|A^{n}_{1},...,A^{n}_{K}) = \frac{P(A^{n}_{1},...,A^{n}_{K}|L^{n})P(L^{n})}{P(A)}
\end{equation}

We make the simplifying assumption without loss of generality that marginal probability of any set of answers $P(A)$ is uniform and only focus on the numerator.  It's already been stated that $P(L)$ is just the CRF's marginal probability before considering any new evidence.  The evidence term can be modeled with the same assumption that was made in using Dawid and Skene to estimate Turker quality.

\begin{equation}
\label{eq:independence}
P(A^{n}_{1},...,A^{n}_{K}|L^{n}) = \prod_{k}P(A^{n}_{k}|L^{n})
\end{equation}

\begin{equation}
\label{eq:bayes_evidence}
P(A^{n}_{k}=a|L^{n}=l) = |\mathbbm{1}_{{a}\neq l} - Q_{k}|
\end{equation}

where $a$ and $l$ are values drawn from the label space and $Q_{k}$ is the quality of the k$^{th}$ worker.  Equation~\ref{eq:independence} follows from all Turker answers being independent of each other and equation~\ref{eq:bayes_evidence} simply restates our assumption about the use of Turkery quality $Q_{k}$.  The probability of a Turker's answer $a$ agreeing with the true label $l$ equals $Q_{k}$, while the probability of an incorrect answer where they disagree is $1-Q_{k}$.  The full model is

\begin{equation}
\label{eq:full_bayes}
P(L^{n}=l|A^{n}_{1}=a_{1},...,A^{n}_{K}=a_{k}) = P(L^{n}=l)\prod_{k}|\mathbbm{1}_{{a_{k}}\neq l} - Q_{k}|
\end{equation}

Using equation~\ref{eq:full_bayes} for all possible labels $l$ and renormalizing produces a new posterior distribution accounting for both the initial ML extracted result and evidence gathered from the crowd.  The product can be extended and updated as new evidence comes in over time.  While currently evidence is designed to come from the crowd in \sysName , there is no explicit restriction preventing future updates from incorporating evidence from a number of different extractions as well as the crowd.

\subsubsection{Dempster-Shafer Evidential Combination}

A viable alternative is to exhibit no faith in the machine's initial marginal calculation.  After all, one could argue that by selecting only the most uncertain tokens that metric loses its value.  Tossing out the CRF prior, we're left with the task of \textit{combining} disparate evidence from a group of Turkers.  This can be accomplished using Dempster's Rule of Combination.  

While the Bayesian approach was inspired by an alternative interpretation of belief functions, the actual implementation is still a probability function through and through, with all of Kolmogorov's axioms defining a probability function still holding.  For evidential combination, however, we leverage the full power of belief theory and relax some of those axioms to map to a set of belief functions.  The main difference between a belief function and a probability function is that probability functions are defined only over the \textit{measurable} subsets of a set while belief functions are defined over \textit{all} subsets (the power set) of a set \cite{shafer1976mathematical}.

Mapping the data from Turkers into belief or mass functions is relatively straightforward.  Like with the Bayesian approach, our confidence in them getting the answer correct is reflected in their Quality score.  The mass function $m(a_{k})$ gets assigned the score $Q_{k}$.  Let $\mathcal{A}$ be the set of all possible labels ${1,2,...,L}$.  Intuitively, $m(\mathcal{A})$ is the mass associated with a random guess and all $L$ labels being equally likely.  We assume in this framework that Turkers are either reliable, getting the answer correct with belief score $Q_{k}$, or unreliable, reflected in a random guess with belief score $1-Q_{k}$.  Explicitly, for a Turker $k$ with provided answer $a_{k}$:

\begin{equation}
m^{n}(2^{L}) = 0
\end{equation}
\begin{equation}
m^{n}(a_{k}) = Q_{k}
\end{equation}
\begin{equation}
m^{n}(\mathcal{A}) = 1-Q_{k}
\end{equation}

The first equation simply states that initialize all mass functions to zero before setting the two values below.  The mass function $m(\mathcal{A}$ has no meaning in standard probability theory, as the set of all outcomes is not a measurable in the probabilistic sense.  We use it mainly as bookkeeping for the uncertainty in the result before normalizing it out when the aggregation computation is completed.  The set of mass functions from multiple Turkers can be combined using Dempster's Rule of Combination between Turker 1 and Turker 2 for each set $A\in2^{L}$:

\begin{equation}
\begin{split}
m_{0,1}(A) &=(m_{1}\oplus m_{2})(A)\\
                   &=\frac{1}{1-K} \sum_{B\cap C=A\neq\emptyset} m_{1}(B)m_{2}(C)
\end{split}
\end{equation}

\begin{equation}
K=\sum_{B\bigcap C=\emptyset}m_{1}(B)m_{2}(C)
\end{equation}

The procedure is to map all HIT responses to mass functions and combine them one-by-one in turn to produce a single combined mass function.  Any remaining uncertainty in $m_{comb}(\mathcal{A})$ is added to all the singleton functions and re-normalized to produce a single probability function.  The original belief formulation is maintained in \sysName for easy combination if new evidence arrives at a later time.

While we introduce Dempster-Shafer theory in the context of our simpler one-answer-per-question framework currently found in \sysName , the method will become more powerful in future work when we plan to extend functionality to allow the Turkers to provide more than one response per question when uncertain.  Reasoning over such fuzzy sets exemplifies the real power for using belief theory over probability theory.

\subsection{Thresholding the Turkers}

Even human computation is not perfect.  The previous section looked at ways to combine Turker answers probabilistically to arrive at a final result that is not deterministic.  This is useful for when there is controversy or confusion elicited over the answers of a question.  Placing a cap on the entropy of the final probability function is a good way to guarantee only high quality returns on crowdsourced answers.  Answer aggregations not falling under the cap have their answers retained and questions are re-submitted during the next batch to gain further evidence in reaching a decision.  

\subsection{Probabilistic Integration}

Not only does \sysName have the ability to aggregate answers from multiple sources, but also the ability to reinsert the resulting distribution back into the CRF. Since the underlying architecture of the system is a CRF, the dependence properties of each field are made explicit and re-running the inference algorithm has the potential to change surrounding fields as well.  This "constrained inference" substitutes the aggregated marginal distribution of a token in for the computed transition probabilities.  This highlights a very strong advantage of \sysName system, in that large errors can be corrected by small, incremental changes.

